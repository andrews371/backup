{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src = os.path.join(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(src)\n",
    "\n",
    "from src import utils\n",
    "\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# método get - recebendo dados da net de forma automática\n",
    "\n",
    "import requests \n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O MÉTODO ACIMA É BEM SIMPLES, NÃO CHEGUEI A IMPLEMENTAR MAS SE QUISER VER\n",
    "# SÓ ABRIR O GMAIL E PESQUISAR POR \"ROBÔ WEB\" E VER OS LINKS. UM DELES ENSINA A PEGAR DADOS DA WEB\n",
    "# USANDO O IMPORT REQUESTS E O CÓDIGO ACIMA ALÉM DE MAIS UM POUCO DE CÓDIGO.\n",
    "# MAS PARA PEGAR DADOS DA WEB ELE TAMBÉM ENSINOU UMA MANEIRA MAIS ROBUSTA QUE ESTÁ EXEMPLIFICADA \n",
    "# ABAIXO, USANDO O PACOTE BEAUTIFULSOUP. PREFERI IMPLEMENTAR DESTA ÚLTIMA FORMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# método post - mandando dados pela net de forma automática\n",
    "\n",
    "import re\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "url = 'http://testing-ground.scraping.pro/login?mode=login'\n",
    "data = {'pwd': '12345', 'usr': 'admin'}\n",
    "\n",
    "headers = {'User-Agent': ''}\n",
    "session.get('http://testing-ground.scraping.pro/login')\n",
    "response = session.post(url, data=data, headers=headers)\n",
    "re.findall('WELCOME', response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re # acho que essa não precisava aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://pt.wikipedia.org/wiki/Campeonato_Brasileiro_de_Futebol_de_2019_-_S%C3%A9rie_A'\n",
    "resposta = requests.get(url1)\n",
    "\n",
    "soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "tabela = soup.find_all('table')\n",
    "print(len(tabela))\n",
    "print(tabela[0])\n",
    "print(tabela[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas = pd.read_html(str(soup), header = 0)\n",
    "len(tabelas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = tabelas[3]\n",
    "dados.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'https://pt.wikipedia.org/wiki/Ubuntu'\n",
    "resposta = requests.get(url)\n",
    "\n",
    "soup = bs(resposta.text, 'html.parser')\n",
    "tabela = soup.find_all('p')\n",
    "print(tabela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'https://future.pinnacle.com/en'\n",
    "resposta = requests.get(url)\n",
    "with open('pin.html', 'wb') as arq:\n",
    "    arq.write(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'https://pt.wikipedia.org/wiki/Ubuntu'\n",
    "resposta = requests.get(url)\n",
    "\n",
    "page = bs(resposta.text, 'html.parser')\n",
    "conteudo = page.find_all('a')\n",
    "# print(conteudo, '\\n') aqui é o conteúdo da linha anterior da tag \"a\"\n",
    "print(conteudo[4], '\\n') # esse conteúdo já vem de um \"find (find_all no caso) e posso especificar com \"[4]\"\n",
    "print(conteudo[4].find('abbr'), '\\n') # eu posso ir de um \"find\" e usar \"[]\" e dá outro \"find\" e usar \"[]\" novamente\n",
    "                                # e ir entrando mais na página e especificando mais ainda o que eu quero\n",
    "\n",
    "print(conteudo[4].find('abbr').text, '\\n') # nesses prints qnd cheguei onde queria, ainda mandei exibir só os textos\n",
    "print(page.find_all('p')[3].text)\n",
    "print(page.find('body').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "# do requests temos os métodos, get, post, put, delete. Para ver como funciona pesquisar\n",
    "# se eu só vou usar o método get do requests, eu poderia apenas fazer:\n",
    "# from requests import get\n",
    "# e no lugar do códgigo abaixo: resposta = requests.get(url)\n",
    "# eu faria apenas: resposta = get(url)\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'https://pt.wikipedia.org/wiki/Ubuntu'\n",
    "resposta = requests.get(url)\n",
    "print(f'A reposta do servidor é -> {resposta} \\n\\n')\n",
    "print(f'O código da reposta do servidor é -> {resposta.status_code} \\n\\n')\n",
    "print(f'O cabeçalho do servidor é -> {resposta.headers} \\n\\n')\n",
    "print(f'Os cookies do servidor é -> {resposta.cookies} \\n\\n')\n",
    "print(f'O texto do servidor que vamos usar com o BeautifulSoup é -> {resposta.text} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = bs(resposta.text, 'html.parser')\n",
    "\n",
    "# a tag class que está dentro de table tbm pode ser especificada assim: \n",
    "#conteudo = page.find_all('table', class_='cquote')\n",
    "conteudo = page.find('table', {'class': 'cquote'})\n",
    "print('Aqui começa o uso do BeautifulSoup \\n\\n')\n",
    "print(f'print direto no objeto page que recebe o BeautifulSoup {page} \\n\\n')\n",
    "print('Usando o \"find\" e o \"text\" \\n')\n",
    "print(conteudo.text)\n",
    "\n",
    "# se usássemos o \"find_all\" então para imprimirmos como texto, deveríamos dizer a posição\n",
    "# do elemento mesmo que só tivesse 1. Neste caso usaríamos a posição \"[0]\" que é a primeira posição\n",
    "# e como só temos 1 elemento, ele está na primeira posição. Se tivéssemos mais, faríamos \"[1]\" \"[2]\" e assim\n",
    "# por diante. Veja o mesmo código usando o \"find_all\" e \"text\"\n",
    "\n",
    "print('Agora usando o \"find_all\" e o \"text\" \\n')\n",
    "conteudo = page.find_all('table', {'class': 'cquote'})\n",
    "print(conteudo[0].text)\n",
    "\n",
    "# temos o get tbm para pegar atributos de uma tag assim como outros métodos do BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como essa página usa javascript, não funciona só com o requests\n",
    "# nem importando o \"time\" e dando um sleep para carregar a página. Já testei \n",
    "# e pra testar novamente é só dá descomentar as duas linhas de códigos\n",
    "# comentadas abaixo\n",
    "\n",
    "# import time\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'https://future.pinnacle.com/en/'\n",
    "resposta = get(url)\n",
    "# time.sleep(20)\n",
    "page = bs(resposta.text, 'html.parser')\n",
    "print(page.find_all('span')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ao vivo\n",
      "\n",
      "Tunisia\n",
      "Nigeria\n",
      "Crusaders Newtownabbey Strikers (w)\n",
      "Glentoran Belfast United LFC (w)\n",
      "Cortulua (w)\n",
      "AC Deportivo Cali (w)\n",
      "KB Breidholt\n",
      "KM Reykjavik\n",
      "Berserkir\n",
      "Lettir Reykjavik\n",
      "\n",
      "Destaques\n",
      "\n",
      "Borussia Monchengladbach\n",
      "Schalke 04\n",
      "Celta Vigo\n",
      "Real Madrid\n",
      "Athletic Club Bilbao\n",
      "Barcelona\n",
      "Atletico Madrid\n",
      "Getafe\n",
      "Manchester United\n",
      "Chelsea\n",
      "West Ham United\n",
      "Manchester City\n",
      "Tottenham Hotspur\n",
      "Aston Villa\n",
      "Newcastle United\n",
      "Arsenal\n"
     ]
    }
   ],
   "source": [
    "# Aqui mesmo a página usando javascript conseguimos fazer web scraping \n",
    "# para isso usamos o selenium\n",
    "\n",
    "# Aqui o firefox abre quando começar a executar o selenium\n",
    "# serve como depuração tbm pra ver se está respondendo como deveria\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://future.pinnacle.com/en/'\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(url)\n",
    "time.sleep(15)\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page_bs = bs(html, 'html.parser')\n",
    "\n",
    "erro_1 = 0\n",
    "erro_2 = 0\n",
    "\n",
    "try:\n",
    "    tam_ao_vivo = len(page_bs.find('div', {'data-test-id':'LiveContainer'}).find_all('span', {'class':'_V4AXE'}))\n",
    "    ao_vivo = page_bs.find('div', {'data-test-id':'LiveContainer'}).find_all('span', {'class':'_V4AXE'})\n",
    "except:\n",
    "    erro_1 = 1\n",
    "    \n",
    "try:\n",
    "    tam_destaques = len(page_bs.find('div',{'data-test-id':'Highlights-Container'}).find_all('span', {'class':'_V4AXE'}))\n",
    "    destaques = page_bs.find('div',{'data-test-id':'Highlights-Container'}).find_all('span', {'class':'_V4AXE'})\n",
    "except:\n",
    "    erro_2 = 1\n",
    "\n",
    "if erro_1 == 0:\n",
    "    print('Ao vivo\\n')\n",
    "    for i in range(tam_ao_vivo):\n",
    "        print(ao_vivo[i].text)\n",
    "\n",
    "if erro_2 == 0:\n",
    "    print('\\nDestaques\\n')\n",
    "    for i in range(tam_destaques): \n",
    "        print(destaques[i].text)                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://www.betfair.com/exchange/plus/football/inplay/'\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(url)\n",
    "time.sleep(15)\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page = bs(html, 'html.parser')\n",
    "tamanho = len(page.find_all(class_='name'))\n",
    "for i in range(tamanho):\n",
    "    times = page.find_all(class_='name')\n",
    "    print(times[i].text)\n",
    "driver.quit() # aqui ao final fecha o navegador que estava aberto e visível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para usar o selenium com firefox invísivel use o código abaixo\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# local onde se encontra o instalador do geckdriver\n",
    "geckodriver = '/home/andre/Downloads/geckodriver'\n",
    "\n",
    "driver = webdriver.FirefoxOptions()\n",
    "driver.add_argument('-headless')\n",
    "\n",
    "driver = webdriver.Firefox(executable_path=geckodriver, options=driver)\n",
    "\n",
    "driver.get('https://www.betfair.com/exchange/plus/football/inplay')\n",
    "time.sleep(15)\n",
    "driver.save_screenshot('/home/andre/Downloads/andre.png')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlético-MG\n",
      "Cruzeiro MG\n",
      "Bahia\n",
      "Grêmio\n",
      "Internacional\n",
      "SE Palmeiras\n",
      "Flamengo\n",
      "Atletico PR\n",
      "Independiente (Ecu)\n",
      "Caracas\n",
      "New England\n",
      "Vancouver Whitecaps\n",
      "Atlanta Utd\n",
      "Houston Dynamo\n",
      "Toronto FC\n",
      "New York Red Bulls\n",
      "Chicago Fire\n",
      "Columbus\n",
      "Gremio Esportivo Juventus\n",
      "Barra FC Porto Alegre\n",
      "Fluminense de Joinville\n",
      "Camboriu\n",
      "Seattle Sounders\n",
      "Dortmund\n"
     ]
    }
   ],
   "source": [
    "# navegador invisível\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "geckodriver = '/home/andre/Downloads/geckodriver'\n",
    "driver = webdriver.FirefoxOptions()\n",
    "driver.add_argument('-headless')\n",
    "driver = webdriver.Firefox(executable_path=geckodriver, options=driver)\n",
    "driver.get('https://www.betfair.com/exchange/plus/football/inplay')\n",
    "time.sleep(15)\n",
    "\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page = bs(html, 'html.parser')\n",
    "tamanho = len(page.find_all(class_='name'))\n",
    "for i in range(tamanho):\n",
    "    times = page.find_all(class_='name')\n",
    "    print(times[i].text)\n",
    "driver.quit() # fecha o navegador que estava aberto porém invisível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando a página HTLM da pinnacle\n",
    "# posso fazer isso mostrando o navegador ou não\n",
    "# neste caso optei por usar navegação invisível\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "\n",
    "gecko = '/home/andre/Downloads/geckodriver'\n",
    "driver = webdriver.FirefoxOptions()\n",
    "driver.add_argument('-headless')\n",
    "driver = webdriver.Firefox(executable_path=gecko, options=driver)\n",
    "\n",
    "url = 'https://future.pinnacle.com/en'\n",
    "driver.get(url)\n",
    "time.sleep(15)\n",
    "\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page = bs(html, 'html.parser')\n",
    "\n",
    "with open('pinnacle.html', 'w') as arq:\n",
    "    arq.write(html) # aqui tbm pode passar como parâmetro \"page.text\"\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando a página HTLM da betfair\n",
    "# posso fazer isso mostrando o navegador ou não\n",
    "# neste caso optei por usar navegação invisível\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "\n",
    "gecko = '/home/andre/Downloads/geckodriver'\n",
    "driver = webdriver.FirefoxOptions()\n",
    "driver.add_argument('-headless')\n",
    "driver = webdriver.Firefox(executable_path=gecko, options=driver)\n",
    "\n",
    "url = 'https://www.betfair.com/exchange/plus/football/inplay/'\n",
    "driver.get(url)\n",
    "time.sleep(15)\n",
    "\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page = bs(html, 'html.parser')\n",
    "\n",
    "with open('betfair.html', 'w') as arq:\n",
    "    arq.write(html) # aqui tbm pode passar como parâmetro \"page.text\"\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ao vivo\n",
      "\n",
      "KB\n",
      "KM Reykjavik\n",
      "Cortulua (W)\n",
      "Deportivo Cali (F)\n",
      "Berserkir\n",
      "Lettir\n",
      "Kria\n",
      "KFR\n",
      "\n",
      "A seguir\n",
      "\n",
      "Gremio Esportivo Juventus\n",
      "Barra FC Porto Alegre\n",
      "Atlético-MG\n",
      "Cruzeiro MG\n",
      "Bahia\n",
      "Grêmio\n",
      "Atlanta Utd\n",
      "Houston Dynamo\n",
      "New England\n",
      "Vancouver Whitecaps\n",
      "Fluminense de Joinville\n",
      "Camboriu\n",
      "Chicago Fire\n",
      "Columbus\n",
      "Swope Park Rangers\n",
      "Hartford Athletic FC\n",
      "Toronto FC\n",
      "New York Red Bulls\n",
      "Flamengo\n",
      "Atletico PR\n",
      "Independiente (Ecu)\n",
      "Caracas\n",
      "Independiente Santa Fe (W)\n",
      "CD Los Milonarios (W)\n",
      "Internacional\n",
      "SE Palmeiras\n",
      "Edmonton\n",
      "Valour\n",
      "Seattle Sounders\n",
      "Dortmund\n"
     ]
    }
   ],
   "source": [
    "# Aqui mesmo a página usando javascript conseguimos fazer web scraping \n",
    "# para isso usamos o selenium\n",
    "\n",
    "# Aqui o firefox abre quando começar a executar o selenium\n",
    "# serve como depuração tbm pra ver se está respondendo como deveria\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver   \n",
    "\n",
    "url = 'https://www.betfair.com/exchange/plus/football/inplay/'\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# a linha abaixo clica no menu dropdown e a próxima clica em umas das opções que \n",
    "# aparecem após o primeiro clique\n",
    "driver.find_element_by_class_name('group-by-filter').click()\n",
    "driver.find_element_by_css_selector('.expanded > div:nth-child(3) > bf-option:nth-child(2) > span:nth-child(1)').click()\n",
    "\n",
    "time.sleep(15)\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page_selenium = bs(html, 'html.parser')\n",
    "\n",
    "erro_1 = 0\n",
    "erro_2 = 0\n",
    "\n",
    "try:\n",
    "    tam_ao_vivo = len(page_selenium.find_all('tbody')[1].find_all(class_='name'))\n",
    "    ao_vivo = page_selenium.find_all('tbody')[1].find_all(class_='name')\n",
    "except:\n",
    "    erro_1 = 1\n",
    "    \n",
    "try:\n",
    "    tam_a_seguir = len(page_selenium.find_all('tbody')[2].find_all(class_='name'))\n",
    "    a_seguir = page_selenium.find_all('tbody')[2].find_all(class_='name')\n",
    "except:\n",
    "    erro_2 = 1\n",
    "\n",
    "if erro_1 == 0:\n",
    "    print('Ao vivo\\n')\n",
    "    for i in range(tam_ao_vivo):    \n",
    "        print(ao_vivo[i].text)\n",
    "\n",
    "if erro_2 == 0:\n",
    "    print('\\nA seguir\\n')\n",
    "    for i in range(tam_a_seguir):\n",
    "        print(a_seguir[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ao vivo\n",
      "\n",
      "KB\n",
      "KM Reykjavik\n",
      "Cortulua (W)\n",
      "Deportivo Cali (F)\n",
      "Berserkir\n",
      "Lettir\n",
      "Kria\n",
      "KFR\n",
      "\n",
      "A seguir\n",
      "\n",
      "Gremio Esportivo Juventus\n",
      "Barra FC Porto Alegre\n",
      "Atlético-MG\n",
      "Cruzeiro MG\n",
      "Bahia\n",
      "Grêmio\n",
      "Atlanta Utd\n",
      "Houston Dynamo\n",
      "New England\n",
      "Vancouver Whitecaps\n",
      "Fluminense de Joinville\n",
      "Camboriu\n",
      "Chicago Fire\n",
      "Columbus\n",
      "Swope Park Rangers\n",
      "Hartford Athletic FC\n",
      "Toronto FC\n",
      "New York Red Bulls\n",
      "Flamengo\n",
      "Atletico PR\n",
      "Independiente (Ecu)\n",
      "Caracas\n",
      "Independiente Santa Fe (W)\n",
      "CD Los Milonarios (W)\n",
      "Internacional\n",
      "SE Palmeiras\n",
      "Edmonton\n",
      "Valour\n",
      "Seattle Sounders\n",
      "Dortmund\n"
     ]
    }
   ],
   "source": [
    "# Aqui mesmo a página usando javascript conseguimos fazer web scraping \n",
    "# para isso usamos o selenium\n",
    "# Aqui não abrimos o firefox\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver   \n",
    "\n",
    "gecko = '/home/andre/Downloads/geckodriver'\n",
    "driver = webdriver.FirefoxOptions()\n",
    "driver.add_argument('-headless')\n",
    "driver = webdriver.Firefox(executable_path=gecko, options=driver)\n",
    "\n",
    "url = 'https://www.betfair.com/exchange/plus/football/inplay/'\n",
    "driver.get(url)\n",
    "time.sleep(2) # tempo para que o menu seja carregado e possa ser clicado\n",
    "\n",
    "# a linha abaixo clica no menu dropdown e a próxima clica em umas das opções que \n",
    "# aparecem após o primeiro clique\n",
    "driver.find_element_by_class_name('group-by-filter').click() \n",
    "driver.find_element_by_css_selector('.expanded > div:nth-child(3) > bf-option:nth-child(2) > span:nth-child(1)').click()\n",
    "\n",
    "# tempo para processamento do javascript da página\n",
    "time.sleep(15)\n",
    "html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "page_selenium = bs(html, 'html.parser')\n",
    "\n",
    "erro_1 = 0\n",
    "erro_2 = 0\n",
    "\n",
    "try:\n",
    "    tam_ao_vivo = len(page_selenium.find_all('tbody')[1].find_all(class_='name'))\n",
    "    ao_vivo = page_selenium.find_all('tbody')[1].find_all(class_='name')\n",
    "except:\n",
    "    erro_1 = 1\n",
    "    \n",
    "try:\n",
    "    tam_a_seguir = len(page_selenium.find_all('tbody')[2].find_all(class_='name'))\n",
    "    a_seguir = page_selenium.find_all('tbody')[2].find_all(class_='name')\n",
    "except:\n",
    "    erro_2 = 1\n",
    "\n",
    "if erro_1 == 0:\n",
    "    print('Ao vivo\\n')\n",
    "    for i in range(tam_ao_vivo):    \n",
    "        print(ao_vivo[i].text)\n",
    "\n",
    "if erro_2 == 0:\n",
    "    print('\\nA seguir\\n')\n",
    "    for i in range(tam_a_seguir):\n",
    "        print(a_seguir[i].text)\n",
    "        \n",
    "driver.quit() # fecha o navegador que estava aberto e invisível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
